# -*- coding: utf-8 -*-
"""Project_1_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbG5wKHWaAVjDoGsBXo6SQnkuFI3VJ_b

Title of the Project is Bank customer churn Model
"""

#Problem   : Bank customer churn
#objective : The objective of the Bank Customer Churn Model project is to leverage Python for analyzing customer data to predict potential churn.
#The goal is to identify patterns and factors contributing to customer attrition, enabling the bank to implement targeted retention strategies and enhance customer satisfaction.
#Data source : https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

df = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

df.head()

df.info()

df.duplicated('CustomerId').sum()

df = df.set_index('CustomerId')

df.info()

#Encoding

df['Geography'].value_counts()

df.replace({'Geography': {'France': 2, 'Germany':1, 'Spain' :0}}, inplace=True)

df['Gender'].value_counts()

df.replace({'Gender' :{'Male': 0, 'Female':1}}, inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num of Products':{1: 0, 2:1, 3:1, 4:1 }}, inplace=True)

df['Has Credit Card'].value_counts()

df['Is Active Member'].value_counts()

df.loc[(df['Balance']==0),'Churn'].value_counts()

df['Zero Balance'] = np.where(df['Balance']>0,1,0)

df['Zero Balance'].hist()

df.groupby(['Churn','Geography']).count()

#Define Label and Features

df.columns

x = df.drop(['Surname','Churn'], axis = 1)

y = df['Churn']

x.shape, y.shape

#Handling Imbalance Data
#like fraud detection
#spam filterwarnings
#Disease Screening
#online sales churn
#Advertising click-throughs
#by undersampling & oversampling

df['Churn'].value_counts()

sns.countplot(x = 'Churn', data = df)

x.shape, y.shape

#Random Under Sampling

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=2529)

x_rus, y_rus = rus.fit_resample(x,y)

x_rus.shape, y_rus.shape, x.shape, y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind = 'hist')

#Random over sampling

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=252)

x_ros, y_ros = ros.fit_resample(x, y)

x_ros.shape, y_ros.shape, x.shape, y.shape

y.value_counts()

y.value_counts()

y_ros.value_counts()

y_ros.plot(kind = 'hist')

#Train Test Split
from sklearn.model_selection import train_test_split

#Split Original Data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2529)

#split Random under Sample Data
x_train_rus, x_test_rus, y_train_rus, y_test_rus = train_test_split(x_rus, y_rus, test_size=0.3, random_state=2529)

#split Random over Sample Data
x_train_ros, x_test_ros, y_train_ros, y_test_ros = train_test_split(x_ros, y_ros, test_size=0.3, random_state=2529)

#Standardize Features

from sklearn.preprocessing import StandardScaler

Sc = StandardScaler()

#standardize Original Data

x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = Sc.fit_transform(x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

#Standardize random Under Sample Data

x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = Sc.fit_transform(x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

#Standardize random over Sample Data

x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = Sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = Sc.fit_transform(x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

#support Vector Machine Classifier

from sklearn.svm import SVC

svc = SVC()

svc.fit(x_train, y_train)

y_pred = svc.predict(x_test)

#Model Accuracy

from sklearn.metrics import confusion_matrix, classification_report

confusion_matrix(y_test, y_pred)

print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, cv = 2)

grid.fit(x_train, y_train)

print(grid.best_estimator_)

grid_predictions = grid.predict(x_test)

confusion_matrix(y_test, grid_predictions)

print(classification_report(y_test, grid_predictions))

#model with random under sampling

svc_rus = SVC()

svc_rus.fit(x_train_rus, y_train_rus)

y_pred_rus = svc_rus.predict(x_test_rus)

#model accuracy

confusion_matrix(y_test_rus, y_pred_rus)

print(classification_report(y_test_rus, y_pred_rus))

#Hyperparameter Tunning

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_rus = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, cv = 2)
grid_rus.fit(x_train_rus, y_train_rus)

print(grid_rus.best_estimator_)

grid_predictions_rus = grid_rus.predict(x_test_rus)

confusion_matrix  (y_test_rus, grid_predictions_rus)

print(classification_report(y_test_rus, grid_predictions_rus))

#model with random over sampling

svc_ros = SVC()

svc_ros.fit(x_train_ros, y_train_ros)

y_pred_ros = svc_ros.predict(x_test_ros)

#model accuracy

confusion_matrix(y_test_ros, y_pred_ros)

print(classification_report(y_test_ros, y_pred_ros))

#hyperparameter tunning

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_ros = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, cv = 2)
grid_ros.fit(x_train_ros, y_train_ros)

print(grid_ros.best_estimator_)

grid_predictions_ros = grid_ros.predict(x_test_ros)

confusion_matrix(y_test_ros, grid_predictions_ros)

print(classification_report(y_test_ros, grid_predictions_ros))